\chapter{Methodology}

\section{Participants}

A total number of \todo{26} participants, \todo{\# females and rest} with the average age of \todo{average age} with the standard deviation of \todo{sd age} took part in the experiment. The participants were all students of the University of Osnabrück. Before the start of the first exploration session all participant gave their written consent for taking part in the experiment (see appx. \ref{appx:forms}, Einverständniserklärung). The participation was voluntary and only students with no health issue were selected (see appx. \ref{appx:forms}, Anamnese). The participants were compensated by test-subject hours and/or 5€ per hour.  

Due to the Covid-19 pandemic sessions were conducted according to the laboratory hygiene regulations with a mask and under 3G rule.

3 participants were excluded due to not being able to comply with the experimental requirements, i.e., come in less than 3 days and more than 4 hours apart.



\section{Experimental Design}

\subsection{City}

This study is conducted in a virtual reality (VR) city with an area of about 1 km\textsuperscript2. The city is consisted of 284 buildings. 56 buildings are used in the experimental task of this study from which 4 are global landmarks, 26 are {\emphasize context meaningful} locations, e.g., shops, construction sites, and 26 are residential, {\emphasize not context meaningful} buildings. These 56 buildings have human agents in front of them and an artwork on one of their walls. Avatars belonging to shops take the pose of an act according to the functionality of that store {\emphasize(meaningful)}, e.g., has a book in the hand in front of a bookstore, or are just standing in front of the building {\emphasize(standing avatar)}. The artworks on the shops are also depicting the kind of the shop.

\todo{
	- FIG: include a map \\
	- include photos of the city
}

\subsection{Application and Technology}

The application of the experiment is implemented utilizing unity version 2019.4.11f1. The assets of the city, e.g., buildings, streets were obtained from a previous study called SpaRe, made also at the university of Osnabrück. They were modified with blender version 2.83 LTS (Long Term Support), as were also the human agents picked from Adobe Mixamo collection. They were modified for this experiment in a way that some contextual objects were added to the human agents in front of context meaningful buildings with regard to the context of the building. \\
The experiment consisted of two separate parts, i.e., Exploration and Testing.  Each part had the option to choose the language of the instructions, i.e., German and English. The experiment was conducted using a HTC Vive Pro Eye VR-Headset. For the virtual moving purposes inside the virtual city the participants were given Index valve controller to navigate inside a city by moving the joystick of the controller. They had both the right and left controllers to be able to use their dominant hand.

\section{Experimental Procedure}

For both parts of the experiments participants were seated on a backless rotating chair to enables them to physically rotate in the virtual city. Any forward, backward and sideways movement were done utilizing the controllers.

\subsection{Exploration}

The exploration consisted of 5 sessions. The sessions had to be no more than 3 days and no less than 4 hours apart. \\
The total duration of each session was 30 minutes broke down into 10 minutes segments for breaks to reduce the possibility of motion sickness. Before starting each segment the built-in eye-tracker of the VR-Headset was calibrated and validated. \\

After inserting participant-ID and choosing the preferred language of the participant the exploration session started with a tutorial. The tutorial was held in a scene separate from the main city. The purpose of the tutorial was to allow the participant to move around, get acquainted with the controller and practice the possible movement options the experiment allowed for. After participants confirmed their confidence in using the controllers the experiment was continued to the exploration session. In the main city participants were advised to explore the city freely.

\subsection{Testing}

No longer than 3 days and no less than 4 hours after the last exploration session the testing session was conducted. The testing comprised of just one session of approximately 2 hours. The testing starts after inputting the participant-ID and choosing the language. There is then a tutorial scene outside of the main city used in the experiment for participants to get acquainted with how to use the controller for performing the tasks. Before presenting the trials and in intervals of 112 trials eye-tracker was calibrated and validated. \\

The testing comprised 336 trials. There were 28 starting locations to perform the pointing task from in the city. The distribution of the starting locations can be seen in figure \ref{fig:starting_locs}. At each starting location 12 target buildings were randomly chosen from a pull of 112 targets (56 task buildings with and without avatars). Some examples are shown in figure \ref{fig:target_photos}. Each of the four conditions of the experiment, i.e., context meaningful with human agent present {\emphasize(CmA)}, context meaningful with no human agent present {\emphasize(CmANo)}, non-context meaningful with human agent present {\emphasize(Sa)}, non-context meaningful with no human agent present {\emphasize(SaNo)} built up 25\% of the trials.  The starting locations themselves were consistent for all participants and their order of execution was randomized for each participant. All movements except the rotation were blocked for the whole testing session to maintain the consistence of the participants' position in the starting locations between participants. \\


\begin{figure}[h]
	\centering
	\includegraphics[width=80mm]{figures/starting_locations.png}
	\caption{distribution of the starting locations in the city.}
	\label{fig:starting_locs}
\end{figure}



\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{figures/45_S(green store)_A.PNG}
		\caption{CmA}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{figures/02_S(boulangerie)_noA.PNG}
		\caption{CmANo}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{figures/05_S(Maraz cafe)_A.PNG}
		\caption{CmA}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{figures/07_R(bear)_noA.PNG}
		\caption{SaNo}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{figures/25_R(alogator)_A.PNG}
		\caption{Sa}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{figures/54_R(tree)_noA.PNG}
		\caption{SaNo}
	\end{subfigure}

	\caption{Examples of photos shown in the pointing task as target buildings. A selection of all four experimental conditions.\\
	\scriptsize{
		\textbf{CmA:}   context meaningful with human agent present\\
		\textbf{CmANo:} context meaningful without human agent present\\
		\textbf{Sa:}    not context meaningful with human agent present\\
		\textbf{SaNo:}  not context meaningful without human agent present}
}
	\label{fig:target_photos}
\end{figure}


Each trial was a pointing task performed from the 28 different starting locations spread out through the city. At each starting location the 12 consecutive trials were performed. In each trial a photo of one of the task buildings (with or without human agent in front of it) was presented at the top center of the screen \todo{(see figure X)}. The participants had the option to press a button to bring the picture to the middle of the screen \todo{(see figure X)} and as soon as the button was released the photo moved back to the upper center part of the screen. Since there was no visual virtual body, there was a green dashed laser beam \todo{(see figure X)} attached to the virtual hand of the participants that moved as they moved their hand. The purpose of the laser beam was to assist the participants with the visualization of the direction they are pointing at. The maximum duration of each trial was 30 seconds. If there was no answer given to the task after 20 seconds from the start of the trial, i.e., no direction was selected by the participant indicating in which direction the target building is located from their current location, a countdown timer appeared on the bottom center of the screen \todo{(see figure X)} and terminated the trial after 10 seconds if there was still no answer given. \\

Selecting an answer for the task, i.e., selecting the direction of the target building was possible with a button press. With once pressing the button it locked the laser beam onto a direction and detached it from the hand \todo{maybe a figure of the controller with the info?}. The participants had the option to either confirm the chosen direction with the same button or cancel it with another. Moving on to the next trial was the result of either the participant confirming a direction or by running out of time. Behavioral and technical data, e.g., the chosen direction, participant position and rotation, reaction times were gathered during each trial. The human agents were present during the testing in the city at their exact designated positions and poses in the exploration sessions. A gray screen fade out and fade in occurred while transporting the participants from their current location to another starting location. This was to serve the purpose of decreasing the chance of motion sickness and also avoiding leaking environmental information while moving in the city.

\todo{
	- include photo of the trial  \\
}


\section{Analysis method}

The data of this experiment is gathered from unity in JSON format. All the further processes for analysis were done in Python \todo{ref} \footnote{Programming language Python: \href{https://www.python.org}{https://www.python.org} - version 3.8}. \\
After importing and converting the data into a Pandas \footnote{Python's Pandas {\emphasize (pd)} library: \href{https://pandas.pydata.org}{https://pandas.pydata.org} - version 1.4.2} dataframe, the preprocessing is done to prepare the data for analysis. In this process the dependent variable {\emphasize absolute\_180\_angle} is derived. The variable contains the absolute angular error between the participant's chosen direction and the actual target position. 

\subsection{Preprocessing}

Different functions of Pandas and Numpy {\emphasize (np)} \footnote{Python's Numpy library: \href{https://numpy.org/}{https://numpy.org/} - version 1.22.3} used for the preprocessing. In all the calculations involving directions and positions, only the right direction (x) and the forward direction (z) are taken into account. \\
The main preprocessing steps are as follow:

\begin{enumerate}
	\item Removed irrelevant columns.
	\item Merged the dataframe containing the information about the starting locations with the main dataframe gathered from the participants.
	\item Calculated the absolute angular error between participant's chosen direction and the actual location of the target building.
	
	\begin{enumerate}
		%\item Calculating the absolute Euclidean distance between the participant’s position and the target building. 
		\item Translated the target building's center position {\emphasize(Tpos)} by the participant's body position {\emphasize(Ppos)} to be able to derive the translated building center position {\emph{Tpos\_t}} take the body position as the origin (0,0).
			\begin{center}
				$Tpos\_t_{x,z}= Tpos_{x,z} - Ppos_{x,z}$
			\end{center}
		The direction vector of participant's chosen direction doesn't need to be translated because unity's output for a direction is a normalized vector and therefore it's origin lies already at (0,0).\\
		
		\item Now that body position is at (0,0) with respect to the translated building center position, the angle difference between the participant's chosen direction {\emphasize(Cdir)} to the positive x-axis and the translated building center position {\emphasize(Tpos\_t)} to the positive x-axis were calculated using Numpy's arctan2(z, x) \todo{ref} \footnote{https://numpy.org/doc/stable/reference/generated/numpy.arctan2.html\#r73eacd397847-1} function. This function calculates the angle in radian between the positive x-axis and the vector given to the function as parameter \todo{cite arctan2}. \\
		Since due to the translation done in step \emph{a} the body position was implicitly translated to the origin (0,0), i.e., subtracting body position from body position results in (0,0), it was possible to pass the z and x coordinates of the {\emphasize Tpos\_t} and the {\emphasize Cdir} to the arctan2 function separately to calculate the angle between the {\emphasize Cdir} and the positive x-axis {\emphasize (Cdir\_to\_x)} and the {\emphasize Tpos\_t} and the positive x-axis {\emphasize (Tpos\_to\_x)}. The results are directly translated to degree utilizing Numpy's rad2deg() \todo{ref} \footnote{https://numpy.org/doc/stable/reference/generated/numpy.rad2deg.html} function.
		
		\begin{center}
			$Tpos\_to\_x_{\theta} = np.rad2deg(np.arctan2(Tpos\_t_{z}, Tpos\_t_{x}))$
		\end{center}
	
		\begin{center}
			$Cdir\_to\_x_{\theta} = np.rad2deg(np.arctan2(Cdir_{z}, Cdir_{x}))$
		\end{center}
		
	\end{enumerate}
\end{enumerate}


\subsection{Visualization}

The visualization is done utilizing Matplotlib \footnote{Python's Matplotlib library: \href{https://matplotlib.org}{https://matplotlib.org} - version 3.5.1} and Seaborn \footnote{Python's Seaborn library: \href{https://seaborn.pydata.org}{https://seaborn.pydata.org} - version 0.11.2} libraries.

\subsection{Analysis}
Due to the hierarchical structure of the data, Linear Mixed Models (LMM) were chosen as the method of analysis. For that the Statmodels \footnote{Python's Statmodels library: \href{https://www.statsmodels.org/stable/index.html}{https://www.statsmodels.org/stable/index.html} - version 0.13.2} library is used.

\todo{
	- trial removal criteria (timeout) \\
	- linear mixed models parameters
}

