\chapter{Methodology}

This thesis is a small part of the Human-A project, a Ph.D. project of M.Sc. Tracy Sanchez Pacheco, at the Neuroinformatics department of Cognitive Science institute of the University of Osnabrück. The project contains different M.Sc. and B.Sc. theses work. 

\section{Participants}

A total number of 23 participants (12 male, mean age of 23.1 years, SD = 4.1) took part in the experiment. The participants were all students of the University of Osnabrück. Before the start of the first exploration session all participants were informed about the procedure of the experiment and gave their written consent to taking part in the study (see appx. \ref{appx:forms}, Einverständniserklärung). The participation was voluntary and only students with no health issues were selected (see appx. \ref{appx:forms}, Anamnese). The participants were compensated by test-subject hours and/or 5€ per hour.  

Due to the Covid-19 pandemic sessions were conducted according to the laboratory hygiene regulations with a mask and under the 3G rule.

3 participants were excluded due to not being able to comply with the experimental requirements, i.e., attending the different sessions in less than 3 days and more than 4 hours apart.

\section{Data gathering}
The data used in this work is gathered with the hard work of Tracy Sánchez Pacheco, Kaya Gärtner, and Mohammad Ghorbani.

\section{Experimental Design}

\subsection{City}

This study was conducted in a virtual reality (VR) city with an area of about 1 (virtual) km\textsuperscript2. The city (see figure \ref{fig:city}) consisted of 284 buildings. 56 buildings were used in the experimental task of which 4 were global landmarks, 26 were {\emphasize context meaningful} locations, e.g., shops, construction sites, and 26 were residential, {\emphasize not context meaningful} buildings. These 56 buildings had human agents in front of them and some artwork on one of their walls. Human agents belonging to the meaningful areas took the pose of an act according to the functionality of that store {\emphasize(meaningful)} (see figure \ref{fig:target_photos_a}), e.g., had a book in the hand in front of a bookstore, or were just standing in front of the residential building {\emphasize(standing human agent)} (see figure \ref{fig:target_photos_c}). 

A sun with a detectable origin was avoided in the city, no street was named and no building was numbered to implicitly direct the participants to prioritize their spatial learning. Furthermore, there were borders around the city so that participants cannot exit the city area.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=100mm]{figures/city.jpg}
	\includegraphics[width=100mm]{figures/city_1.jpg}
	\caption[The virtual city]{the virtual city}
	\label{fig:city}
\end{figure}

\subsection{Application and Technology}

The experiment was implemented with unity game engine \autocite{haas2014history} version 2019.4.11f1. The assets of the city, e.g., buildings, and streets were obtained from a previous study called SpaRe, made also at the University of Osnabrück. Buildings and streets were built with blender \autocite{Hess:2010:BFE:1893021} version 2.83 LTS (Long Term Support), as were also the human agents picked from the Adobe Mixamo \autocite{mixamo} collection. They were modified to interact with an object that underlined the context (meaningfulness) of the area, i.e., holding a book in front of a book store. 

The experiment consisted of two separate parts, i.e., exploration and testing.  Each section had the option to choose the language of the instructions, i.e., German and English. The experiment was conducted using an HTC Vive Pro Eye VR Headset. For the virtual moving purposes inside the virtual city, the participants were given an Index valve controller to navigate inside a city by moving its joystick. They had the option of choosing between right or left controller according to their handedness preference.

\section{Experimental Procedure}

Participants were seated on a backless rotating chair that enabled them to physically rotate in the virtual city. Any forward, backward, and sideways movements were done utilizing the controllers.

\subsection{Exploration}

The exploration consisted of 5 sessions. The sessions had to be no more than 3 days and no less than 4 hours apart. 

The total duration of each session was 30 minutes broken down into 10 minutes segments for breaks to reduce the possibility of motion sickness. Before starting each segment the built-in eye-tracker of the VR Headset was calibrated and validated. 

After inserting the participant ID and choosing the preferred language the exploration session started with a tutorial. The tutorial was held in a scene separate from the main city. The purpose of the tutorial was to allow the participant to move around, get acquainted with the controller, and practice the possible movement options the experiment allowed for. After participants confirmed their confidence in using the controllers the experiment was continued to the exploration session. In the main city, participants were advised to explore the city freely.

\subsection{Testing}

Testing comprised of one session of approximately 2 hours. The testing started after inputting the participant ID and choosing the language. Testing continued with a tutorial scene outside of the main city used in the experiment for participants to get acquainted with how to use the controller for performing the tasks. 

Testing was a pointing task comprising 336 trials performed from 28 different starting locations in the city (see figure \ref{fig:starting_locs}). At each starting location, 12 target buildings were randomly chosen from a pull of 112 targets (56 task buildings with and without human agents) for each participant individually (see figure \ref{fig:target_photos}). 

\begin{figure}[!htb]
	\raggedright
	\includegraphics[width=140mm]{figures/starting_locations_map_color_numbered.png}
	\caption[Distribution of starting locations]{distribution of the starting locations in the city with their IDs used in unity and for the analysis}
	\label{fig:starting_locs}
\end{figure}

Each of the four conditions of the experiment, i.e., context meaningful with human agent present {\emphasize(CmA)}, context meaningful with no human agent present {\emphasize(CmANo)}, non-context meaningful with human agent present {\emphasize(Sa)}, non-context meaningful with no human agent present {\emphasize(SaNo)} built up 25\% of the trials.  The starting locations themselves were consistent for all participants and their order of execution was randomized for each participant. 

All movements except the rotation were blocked for the whole testing session to maintain the consistency of the participant's position in the starting locations between participants. 

\begin{figure}[!htb]
	\centering
	\begin{subfigure}[b]{0.48\linewidth}
		\includegraphics[width=\linewidth]{figures/45_S(green store)_A.PNG}
		\caption{CmA}
		\label{fig:target_photos_a}
	\end{subfigure}
	\begin{subfigure}[b]{0.48\linewidth}
		\includegraphics[width=\linewidth]{figures/45_S(green store)_noA.PNG}
		\caption{CmANo}
		\label{fig:target_photos_b}
	\end{subfigure}
	
	
	\begin{subfigure}[b]{0.48\linewidth}
		\includegraphics[width=\linewidth]{figures/44_Sa.PNG}
		\caption{Sa}
		\label{fig:target_photos_c}
	\end{subfigure}
	\begin{subfigure}[b]{0.48\linewidth}
		\includegraphics[width=\linewidth]{figures/44_SaNo.PNG}
		\caption{SaNo}
		\label{fig:target_photos_d}
	\end{subfigure}

	\caption[Target buildings]{Examples of photos shown in the pointing task as target buildings. A selection of all four experimental conditions.
	\scriptsize{
		\textbf{CmA:}   context meaningful with human agent present.
		\textbf{CmANo:} context meaningful without human agent present.
		\textbf{Sa:}    not context meaningful with human agent present.
		\textbf{SaNo:}  not context meaningful without human agent present.}
}
	\label{fig:target_photos}
\end{figure}

At each starting location, 12 consecutive trials were performed. Before the trial began a green circular loading bar {\emphasize (Go cue)} was shown in the middle of the screen for 25ms (see figure \ref{fig:go_cue}). As soon as the bar was complete, it disappeared and a beep sound was played informing the participants of the start of a new trial. 

\begin{figure}[!htb]
	\centering
	\begin{subfigure}[b]{0.3\linewidth}
		\includegraphics[width=\linewidth]{figures/go_cue_30.jpg}
		\caption{go cue loaded at 30 \%}
		\label{fig:go_cue_30}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\linewidth}
		\includegraphics[width=\linewidth]{figures/go_cue_80.jpg}
		\caption{go cue loaded at 80 \%}
		\label{fig:go_cue_80}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\linewidth}
		\includegraphics[width=\linewidth]{figures/go_cue_full.jpg}
		\caption{go cue completely loaded}
		\label{fig:go_cue_full}
	\end{subfigure}
	
	\caption[Start of trial cue]{The circular loading bar with the duration of 25ms showed as a cue of the start of a new trial
	}
	\label{fig:go_cue}
\end{figure}

In each trial, a photo of one of the task buildings, with or without a human agent in front of it (see figure \ref{fig:target_photos}) was presented at the top center of the screen (see figure \ref{fig:photo_position_top}). The participants had the option to press the trigger button to bring the picture to the middle of the screen (see figure \ref{fig:photo_position_center}). As soon as the button was released the photo moved back to the upper center part of the screen.

\begin{figure}[!htb]
	\centering
	\begin{subfigure}[b]{0.48\linewidth}
		\includegraphics[width=\linewidth]{figures/photo_position_top.jpg}
		\caption{default position, top}
		\label{fig:photo_position_top}
	\end{subfigure}
	\begin{subfigure}[b]{0.48\linewidth}
		\includegraphics[width=\linewidth]{figures/photo_position_center.jpg}
		\caption{optional position, center}
		\label{fig:photo_position_center}
	\end{subfigure}
	
	\caption[Trial's target photo positions]{The default (a) and the optional positioning (b) of target photos in each trial. The dashed green laser beam helped the participants to see where they are pointing at
	}
	\label{fig:photo_positions}
\end{figure}

Since there was no visual virtual body, there was a green dashed laser beam (visible in figures \ref{fig:photo_position_center}) attached to the virtual hand of the participants that moved as they moved their hands. The purpose of the laser beam was to assist the participants with the visualization of the direction they were pointing at. The maximum duration of each trial was 30 seconds. If there was no answer given to the task after 20 seconds from the start of the trial, i.e., no direction was selected by the participant indicating in which direction the target building was located from their current location, a countdown timer appeared on the bottom center of the screen (see figure \ref{fig:timer}) and terminated the trial after 10 seconds if there was still no answer given. 


\begin{figure}[!htb]
	\centering
	\includegraphics[width=100mm]{figures/timer.jpg}
	\caption[The countdown timer]{the countdown timer appears at the bottom of the screen after 20s has passed from the trial}
	\label{fig:timer}
\end{figure}

Selecting an answer for the task, i.e., selecting the direction of the target building was possible with a button press. Once pressing the button it locked the laser beam onto a direction and detached it from the hand. The participants had the option to either confirm the chosen direction with the same button or cancel it with another. Moving on to the next trial was the result of either the participant confirming a direction or running out of time. Behavioral and technical data, e.g., the chosen direction, participant position, and rotation and reaction times were gathered during each trial. The human agents were present during the testing in the city at their exact designated positions and pose during the exploration sessions. A gray screen fade-out and fade-in occurred while transporting the participants from their current location to another starting location. This was to serve the purpose of decreasing the chance of motion sickness and also avoiding leaking environmental information while moving inside the city.


\section{Analysis method}

The data of this experiment was gathered from unity in JSON \footnote{Introducing JSON: \href{https://www.json.org/json-en.html}{https://www.json.org/json-en.html}} format. All the further processes for analysis were done in python \autocite{10.5555/1593511} v3.8. For the preprocessing the pandas \autocite{reback2022pandas, mckinney-proc-scipy-2010}, numpy \autocite{harris2020array} and scipy \autocite{2020SciPy-NMeth} libraries, and for the analysis python's statmodels \autocite{seabold2010statsmodels} module were utilized. Matplotlib \autocite{Hunter:2007} and seaborn \autocite{Waskom2021} were used for the visualizations.

After importing and converting the data into a pandas dataframe, the preprocessing was done to prepare the data for analysis. In this process the dependent variable {\emphasize absolute\_180\_angle} was derived. The variable contained the absolute angular deviation of the participant's chosen direction from the actual target position. 

\subsection{Preprocessing}

Different functions of pandas {\emphasize (pd)} and numpy {\emphasize (np)} were used for the preprocessing. In all the calculations involving directions and positions, only the right direction (x) and the forward direction (z) were taken into account. The up direction (y) was excluded as it is not relevant for the analysis at hand. 

The main preprocessing steps were as follows:

\begin{enumerate}
	\item Removed trials in which the participants did not select any direction. These were the trials that were terminated due to timeout. Hence, only the trials where their respective TimeOut variable was False were kept for the analysis. a total of 20 trials were removed. \\
	
	\item Calculated the participant's chosen direction's absolute angular deviation from the actual location of the target building.
	
	\begin{enumerate}
		\item Translated the target building's center position {\emphasize(Tpos)} by the participant's body position {\emphasize(Ppos)} to be able to derive the translated building center position {\emph{Tpos\_t}} to make the body position as the origin at the (0,0) coordinates.
		
			\begin{align*}
				Tpos\_t_{x,z}= Tpos_{x,z} - Ppos_{x,z}
			\end{align*}
		
		The direction vector of the participant's chosen direction didn't need to be translated because unity's output for a direction was a normalized vector and therefore its origin lied already at (0,0).\\
		
		\item Now that body position was at (0,0) with respect to the translated building center position, the angle difference between the participant's chosen direction {\emphasize(Cdir)} to the positive x-axis and the translated building center position {\emphasize(Tpos\_t)} to the positive x-axis were calculated using numpy arctan2(z, x) \footnote{Numpy arctan2: \href{https://numpy.org/doc/stable/reference/generated/numpy.arctan2.html}{https://numpy.org/doc/stable/reference/generated/numpy.arctan2.html}} function. This function calculated the angle in radian between the positive x-axis and the vector given to the function as a parameter. \\
		
		Due to the translation done in step \emph{a} the body position was implicitly translated to the origin (0,0), i.e., subtracting body position from body position resulted in (0,0), it was possible to pass the z and x coordinates of the {\emphasize Tpos\_t} and the {\emphasize Cdir} to the arctan2 function separately to calculate the angle between the {\emphasize Cdir} and the positive x-axis {\emphasize (Cdir\_to\_x)} and the {\emphasize Tpos\_t} and the positive x-axis {\emphasize (Tpos\_to\_x)}. The results were directly translated to degree utilizing numpy rad2deg() \footnote{Numpy rad2deg: \href{https://numpy.org/doc/stable/reference/generated/numpy.rad2deg.html}{https://numpy.org/doc/stable/reference/generated/numpy.rad2deg.html}} function.
	
		\begin{align*}
			Tpos\_to\_x_{\theta} & = np.rad2deg(np.arctan2(Tpos\_t_{z}, Tpos\_t_{x})) \\
			Cdir\_to\_x_{\theta} & = np.rad2deg(np.arctan2(Cdir_{z}, Cdir_{x}))
		\end{align*}
		
		\item After creating {\emphasize Tpos\_to\_x} and {\emphasize Cdir\_to\_x} the angles were respectively converted to {\emphasize Tpos\_to\_x\_360} and {\emphasize Cdir\_to\_x\_360} in the 360 degree environment.

		\begin{align*}
			Tpos\_to\_x\_360_{\theta} = & \\
			& if & \\
			&& Tpos\_to\_x_{\theta} < 180 \\
			& then & \\
			&& 360 + Tpos\_to\_x_{\theta} \\
			& else & \\
			&& Tpos\_to\_x_{\theta}
		\end{align*}
	
		\begin{align*}
			Cdir\_to\_x\_360_{\theta} = & \\
			& if & \\
			&& Cdir\_to\_x_{\theta} < 180 \\
			& then & \\
			&& 360 + Cdir\_to\_x_{\theta} \\
			& else & \\
			&& Cdir\_to\_x_{\theta}
		\end{align*}
		
		\item Calculated the angular difference between the selected direction {\emphasize (Cdir\_to\_x\_360)} and the target building {\emphasize (Tpos\_to\_x\_360)}. They were directly converted to the signed 2 quadrant environment.
		
			\begin{align*}
				signed\_180\_angles_{\theta} = & \\
				& ((Tpos\_to\_x\_360_{\theta} - Cdir\_to\_x\_360_{\theta}) \\
				& + 180) \% 360 - 180
			\end{align*}
		
	
		\item The final step was to create {\emphasize absolute\_180\_angles}, the dependent variable for the main analysis in LMM. This variable stored the absolute value of the angular differences contained in {\emphasize signed\_180\_angles}. The reason for using absolute values was that the direction of the deviation was not a deciding factor in how accurate the participants performed the pointing task. Taking the absolute values was done with numpy abs() \footnote{Numpy absolute: \href{https://numpy.org/doc/stable/reference/generated/numpy.absolute.html}{https://numpy.org/doc/stable/reference/generated/numpy.absolute.html}} function.
		
		\begin{align*}
			absolute\_180\_angles_{\theta} = np.abs(signed\_180\_angles_{\theta})
		\end{align*}
		
	\end{enumerate}

	\item Calculated the Euclidean distance from the participant's body position to the target building for each trial. This was done using the Euclidean distance function \footnote{Scipy spatial Euclidean distance: \href{https://docs.scipy.org/doc/scipy-1.8.0/reference/generated/scipy.spatial.distance.euclidean.html\#scipy.spatial.distance.euclidean}{https://docs.scipy.org/doc/scipy-1.8.0/reference/generated/scipy.spatial.distance.euclidean.html\#scipy.spatial.distance.euclidean}} of scipy spatial.
	
	\begin{align*}
		body\_to\_target_{dis} = Euclidean\_distance((Ppos_x, Ppos_z), (Tpos_x, Tpos_z))
	\end{align*}

\end{enumerate}



\subsection{Analysis}
Due to the hierarchical structure of the data, Linear Mixed Models (LMM) were chosen as the method of analysis. For that the MixedLM function \footnote{Statsmodels function for linear mixed effects model: \\ \href{https://www.statsmodels.org/dev/generated/statsmodels.regression.mixed\_linear\_model.MixedLM.html}{https://www.statsmodels.org/dev/generated/statsmodels.regression.mixed\_linear\_model.MixedLM.html}} from python's statmodels module were used. Subject ID is the grouping component of all the models.

\subsubsection{Independent variables}

\begin{enumerate}
	\item \textbf{Starting locations:} the 28 different starting locations were spread out through the city. See the full list of the locations in appx. \ref{appx:starting_locations}. \\
	
	\item \textbf{Distance to the target:} this variable is the distance of the participant to the target building at each starting location.
\end{enumerate}

\subsubsection{Dependent variables}

\begin{enumerate}
	\item \textbf{Absolute angular deviation} {\emphasize (absolute\_180\_angles)}: the absolute value of the angular deviation of the direction chosen by the participant from the actual location of the target building shown at each trial. The following model was used to predict this dependent variable. 
	
	\begin{align*}
		absolute \; angular \; error \sim starting \; locations + (1 | subject)
	\end{align*}
	

	\item \textbf{Reaction times} {\emphasize (RT)}: this variable stored the duration between the start of each trial and the time the participants confirmed a direction as their response. The beginning timestamp of the trial was directly after the go cue was completed and the end timestamp instantly after the response was given. Calculating the duration was done in unity. 
	
	\begin{align*}
		RT = end\_timestamp - begin\_timestamp
	\end{align*}

	 The following model were used to predict this dependent variable.
	 
	 \begin{align*}
	 	reaction \; times \sim starting \; locations + (1 | subject)
	 \end{align*}
 
\end{enumerate}


\subsection{Code accessibility}

To access the codes written for the analysis of this thesis, please refer to \textcite{Maleki_The_effect_of_2022}.

